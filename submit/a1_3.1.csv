1,0.350125,0.609543356,0.185942173,0.469599605,0.143422354,0.301599391,0.54294032,0.385551948,0.318681319,1188,60,469,232,911,373,489,233,797,121,950,155,1043,133,556,290
2,0.323375,0.759363776,0.16999003,0.244191794,0.134520277,0.277361319,0.475592748,0.444244604,0.325748503,1480,81,197,191,1324,341,166,175,1164,168,494,197,1368,127,255,272
3,0.38025,0.517188302,0.149052841,0.577360356,0.28041543,0.344262295,0.682648402,0.384590056,0.355040701,1008,16,583,342,733,299,576,398,484,81,1168,290,703,42,710,567
4,0.39525,0.548486403,0.161016949,0.49678695,0.378338279,0.357525084,0.599257885,0.433750539,0.355153203,1069,38,405,437,700,323,421,562,525,103,1005,390,696,75,486,765
5,0.406625,0.465879938,0.356430708,0.508650519,0.297230465,0.368506494,0.448838669,0.443725744,0.370073892,908,291,399,351,550,715,368,373,421,274,1029,299,585,313,523,601
The overall accuracy seems to be increasing with each classifier except for SVM with an RBF kernel. This implies that the more complex ensemble and MLP methods perform better than a single classifier and that projecting the data to a higher dimension (via RBF kernel) doesn't help separation. All classifiers vary in terms of per class Precision/Recall scores. It seems that it is better to avoid failing at one class than achieving slight improvements to all other classes. ,,,,,,,,,,,,,,,,,,,,,,,,,
