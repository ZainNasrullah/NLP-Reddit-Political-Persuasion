5,2.19464740847e-261,9.28179529495e-259,1.2009706037e-254,1.62094104439e-250,1.42708369958e-247
10,2.19464740847e-261,9.28179529495e-259,1.2009706037e-254,1.62094104439e-250,1.42708369958e-247,2.19689709733e-247,4.87052795443e-247,5.39307129486e-247,2.08175243741e-246,3.57325009272e-245
20,2.19464740847e-261,9.28179529495e-259,1.2009706037e-254,1.62094104439e-250,1.42708369958e-247,2.19689709733e-247,4.87052795443e-247,5.39307129486e-247,2.08175243741e-246,3.57325009272e-245,2.47697210264e-244,2.51123602864e-243,8.5095765573e-243,1.34522308426e-242,7.27964219012e-241,1.89452941747e-239,2.56090593925e-239,2.69025888621e-239,3.05424555154e-239,4.71184448373e-238
30,2.19464740847e-261,9.28179529495e-259,1.2009706037e-254,1.62094104439e-250,1.42708369958e-247,2.19689709733e-247,4.87052795443e-247,5.39307129486e-247,2.08175243741e-246,3.57325009272e-245,2.47697210264e-244,2.51123602864e-243,8.5095765573e-243,1.34522308426e-242,7.27964219012e-241,1.89452941747e-239,2.56090593925e-239,2.69025888621e-239,3.05424555154e-239,4.71184448373e-238,6.62244694342e-238,2.97424984255e-236,3.13591675349e-236,4.11132565111e-236,1.96887497579e-235,2.79690161195e-235,8.35628398336e-235,3.21001733533e-234,2.4710551674e-233,1.23860322481e-232
40,2.19464740847e-261,9.28179529495e-259,1.2009706037e-254,1.62094104439e-250,1.42708369958e-247,2.19689709733e-247,4.87052795443e-247,5.39307129486e-247,2.08175243741e-246,3.57325009272e-245,2.47697210264e-244,2.51123602864e-243,8.5095765573e-243,1.34522308426e-242,7.27964219012e-241,1.89452941747e-239,2.56090593925e-239,2.69025888621e-239,3.05424555154e-239,4.71184448373e-238,6.62244694342e-238,2.97424984255e-236,3.13591675349e-236,4.11132565111e-236,1.96887497579e-235,2.79690161195e-235,8.35628398336e-235,3.21001733533e-234,2.4710551674e-233,1.23860322481e-232,3.30154544444e-232,2.46707638595e-231,2.7487290954e-231,1.66427857016e-230,2.68815656528e-230,2.84930683006e-230,7.69253041854e-228,1.14073461974e-227,1.98446036681e-227,7.67293180521e-226
50,2.19464740847e-261,9.28179529495e-259,1.2009706037e-254,1.62094104439e-250,1.42708369958e-247,2.19689709733e-247,4.87052795443e-247,5.39307129486e-247,2.08175243741e-246,3.57325009272e-245,2.47697210264e-244,2.51123602864e-243,8.5095765573e-243,1.34522308426e-242,7.27964219012e-241,1.89452941747e-239,2.56090593925e-239,2.69025888621e-239,3.05424555154e-239,4.71184448373e-238,6.62244694342e-238,2.97424984255e-236,3.13591675349e-236,4.11132565111e-236,1.96887497579e-235,2.79690161195e-235,8.35628398336e-235,3.21001733533e-234,2.4710551674e-233,1.23860322481e-232,3.30154544444e-232,2.46707638595e-231,2.7487290954e-231,1.66427857016e-230,2.68815656528e-230,2.84930683006e-230,7.69253041854e-228,1.14073461974e-227,1.98446036681e-227,7.67293180521e-226,1.08065671845e-222,2.60548509398e-222,7.27514637575e-222,2.27024415236e-219,6.35788558524e-219,1.15312821156e-218,2.35736226138e-217,1.77559133025e-216,4.26359806353e-216,3.99856689243e-215
0.295625,0.305125
It seems that mostly the LIWC/Receptiviti features are chosen at both lower and higher amounts of input data. This suggests that emotional features are less noisy and have a larger contribution to discriminibility even in the presence of limited training data.
The p-values in the 32k training set are much, much smaller than the 1k training set counterpart. This is to be expected as having more data leads to more certainty regarding the contribution towards classification of each feature which is exactly what the p-value measures.
The top five features in my code for the 32K training case are the features existing at indices 101, 86, 36, 31 and 45. These all correspond to LIWC/Receptiviti features. This means that these features provide the most discriminibility within each post which suggests that previously counted features aren't as important for this classification task. Intuitively, counting things such as the number of nouns or verbs wouldn't be as relevant as LIWC which quantify word choice and sentiment in judging which reddit group a post belongs.
